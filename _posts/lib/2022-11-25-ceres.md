---
layout: post
title:  "ceres"
date:   2025-12-23 20:18:44 +0800
categories: [Tech]
excerpt: ceres
tags:
  - ceres
---

# install

### install by apt

install on ubuntu20

```bash
sudo apt install libceres-dev
```

### install from source

##### enable eigen_metis during ceres compilation

prerequisites

1. Ceres version ≥ 2.1.0 (METIS support added in 2.1).
2. Eigen ≥ 3.3.0
3. METIS library installed (required by EIGEN_METIS).

Add configuration to compilation: `-DEIGENSPARSE=ON`, `-DEIGEN_METIS=ON`.

##### enable cuda

Prerequisites

1. CUDA Toolkit ≥ 9.0
2. CMake ≥ 3.5
3. g++ ≥ 7

Add configuration to compilation: `-DCERES_USE_CUDA=ON`

In the CMake output (or CMakeCache.txt), ensure:

```yaml
-- CUDA support     : YES
```

### check ceres

check version of ceres

```bash
sudo cat /usr/local/include/ceres/version.h
```

# usage

### problem

```C++
Problem problem;

// Add residual terms to the problem using the autodiff wrapper to get the derivatives automatically.
problem.AddResidualBlock(new AutoDiffCostFunction<F1, 1, 1, 1>(new F1), nullptr, &x1, &x2);
// AutoDiffCostFunction<函数，残差的数量，第一个参数块，第二个参数块>
```

### options

```C++
ceres::Solver::Options options;

// Do not print the iteration table to stdout
options.minimizer_progress_to_stdout = false;

// Do not log per-iteration progress via glog
options.logging_type = ceres::SILENT;

// If you added any callbacks that print, remove them.
// options.callbacks.clear();

```

### robust loss function

| Loss Function  | Behavior for Large Residuals | Notes                      |
| -------------- | ---------------------------- | -------------------------- |
| `HuberLoss`    | Linear growth                | Mild robustness            |
| `SoftLOneLoss` | Very smooth, gentle          | Often good default         |
| `CauchyLoss`   | Logarithmic growth           | Strong outlier suppression |
| `TukeyLoss`    | Hard rejection               | Non-convex, aggressive     |

##### CauchyLoss

Interpretation of Parameter

Small

1. Strong outlier suppression
2. Risk of discarding valid but large residuals

Large:

1. Behavior closer to standard least squares
2. Weaker robustness

A common practical choice is to set parameter close to the expected inlier residual magnitude. For SLAM / BA, Cauchy is often better than Huber when mismatches exist.

##### HuberLoss
