
-----------------------------------------------------------------------------------------------------------------

###
SVD分解求解DLT是不是要取最小特征值对应的向量？是的


###
用imu_utils标定出来的imu内参是直接填写到orbslam3的配置文件里，还是需要离散化处理呀？
理论上随机游走离散化需要除以根号下采样时间，bias需要乘上根号下采样时间。我看程序里面有处理，噪声乘上了频率开方，随机游走除了这个频率开方。标定应该只能标定噪声方差啊，还能标定偏置吗


###
ORBSLAM中，计算描述子的256个点对pattern是怎么按照高斯分布生成的。我想生成一个8点对的pattern应该怎么做呢？有篇论文专门讲这个的，ORB-SLAM2的论文参考文献里有。应该就是高斯分布采样，我刚看到python有相应采样函数

###


###
单应矩阵描述了两个平面之间的映射关系，因此可以描述纯平面场景，纯旋转的图像因为缺乏t不能用E矩阵和F矩阵，无穷远的物体类似平面物体。<br>
基础矩阵是一个三阶矩阵，最高自由度为9，由于尺度等价性，自由度-1，以及又由于不可逆矩阵性质，自由度再-1；一般用8点法求取，最少可以用7点对应估计基础矩阵<br>
当特征点位于同一平面设计，使用八点法求解E时会退化，纯旋转问题因为没有位移，还是退化问题，同时不满足对极约束，E本身为0<br>
射影变换：相机的成像过程实际是将三维空间的点P=(X,Y,Z)变换到成像平面的过程<br>
欧式变换（等距变换）：相对于旋转+平移；相似变化：欧式变换再加上缩放；仿射变换=相似变换再加上切变，形状会发生变化。但是都不会改变平行线的平行性质。

###
fast提取角点的算法，没有描述子


###
ORBSLAM是纯视觉特征点法，SVO和DSO是纯视觉直接法，OKVIS是特征点和IMU紧耦合的VIO。

###
模糊图像像素梯度小，提取出来的特征点数量会少，会影响slam系统的精度

###
https://blog.tensor-robotics.com/archives/371b1147.html

###
一般算出R，t后， 怎么评价R，t算的好坏哈， 除了把一对匹配的点  对应算回去比较久P1 P1‘的话。<br>
卡方分布，看你评价几个自由度的误差了。<br>
可以算内点数，把特征点再次投影，计算重投影误差，利用卡方检验算内点数，大于一个阈值一般来说认定这个位姿比较准确。<br>
带入后端，非线性优化中，构造最小二乘问题，误差越小约精确

###
LVI-SAM不可以用单线雷达。

###
舒尔补掉三维点剩下相机位姿纬度很小，解出来之后带到舒尔补方程中，三维点是对角阵也很好解

###
为什共面的情况会发生退化呢？3点可以确定平面，有一个点就没有作用了;共面后会降秩的。八点法是指一个图像中的八对点。


###
深度图用的时候一般是用什么格式，我看有用CV_16U的，有用CV_32F的，怎么再转换成米呢？
https://ethanli.blog.csdn.net/article/details/125017920?spm=1001.2014.3001.5502

###
realsense435i，win10识别不到吗？记得用USB3.0接口，见 https://blog.csdn.net/weixin_52797278/article/details/124323313

###
各位大佬，有推荐的点云路沿提取的算法吗？知道有地面的，patchwork，算法里面考虑到curb了，但是不知道能不能提出来

###
为什么四元数的变化量可以写成一四元数相乘啊？四元数表示旋转，两个四元数乘法含义表示依次旋转两次。对一般且随时间变化的群元素，根据群对乘法完备特性，t+delta时刻的群元素q(t+delta)，可以由q(t)乘以另一个群元素得到，把那个群元素命名为 delta q

###
 您说的ndt重定位我没用过，我现在用的是hdl的多线程ndt

https://blog.csdn.net/michellechouu/article/details/113450103

###
请问编译aloam的时候提示 vtkrenderibgcontextopengl   not avalaible。看了一下是find package pcl里面报错的。这要怎么解决呀请问？在pclconfigcmake里面找到这个vtkrenderingcontextopengl 的lib 在后面加上个2就好了。vtk7之后这些lib改名了似乎。

###
有人知道如何标定固态激光雷达和imu的外参吗？livox的。你看fastlio或者r3live，应该都有标定的链接。https://github.com/hku-mars/LiDAR_IMU_Init
。相机和雷达：https://github.com/hku-mars/livox_camera_calib，这个工程上很难用，得找到场景很好的

###
有没有大佬会拼接pcd地图的，GitHub有没有源码？cloudcompare可以；automerge_server，但是还没开源，有github，还没上传代码；ros有个包叫map_merge_3d

###
激光slam的帧间ICP在走廊场景跑不起来吗？因为我做过一些走廊的实验，所以有些自己看法仅供参考。首先要看走廊是怎么样的，纵向约束少到什么程度。有可能全部白墙，也有可能有一些门槛之类的小约束。再次看是不是滤波，比如同一个走廊数据集，用lio-sam测试，如果体素滤波设置大一些，很容易就飘，即使车停下来，建图还在继续。体素小一些，还原度还是很高的。r3live有个degenerate序列，就是走廊，Fastlio 会飞

###
有正在用lio sam做项目的吗？或者3D slam选型的也行。目前室内场景都跑得不好，有个别场景可以跑的挺好。现在也在用hdl graph slam测试，初步测试的几个自采数据，发现这个框架的效果比lio sam的要好些。hdl的缺陷在于算力的消耗

###
那个y9000p跑lio_sam rviz界面很卡有什么解决办法嘛？用独显，csdn搜 y9000p装nvidia驱动

###
有用EKF做视觉和轮速融合的开源方案吗？有个vins on wheels，是结合轮速的

###
计算机视觉中的多视图几何这本书，以本质矩阵和pnp问题为核心，用到什么就看什么就好了，没必要从头看到尾

###
我们使用自己的硬件跑，低速无人车，发现融合imu改善效果不明显，在蛮多情况下反而会建图失败。就直接先不用imu了。推测只有imu和lidar之间和各自的参数标定，也就是传感器处于理想状态，imu才能发挥正向作用

###
vins mono对ceres版本有要求吗？用的应该是1.14吧，其他版本你可以试试。我用的Ubuntu18.04 按照官网的教程点下去，ceres用的是2.1，我感觉太高了

###
想问一下，用zed跑orbslam的时候是只用左右目的图像还是用相机算出来的深度图呀，有人试过那种效果好吗？左右目就是双目模式，计算的深度图就是rgbd 模式，两种感觉都差不多

###
Ubuntu升级到22.04跑《SLAM十四讲》里面的代码会碰到各种各样的问题。我用的20，infinitam、elasticfusion、bundlefusion、badslam全都跑通了。当然不可避免，需要自己修改一些东西，但是绝大部分问题网上都能找得到的

###
组合导航和三个激光雷达，一般道路上（只有道路灌木树，没有墙）如何评估建图效果啊？相当于评估融合建图结果，其实是为了验证雷达间外参，需要能检测出1度的误差。如果惯导的结果精度足够 可以用来当真值；点云地图有个叫 地图熵的东西，有些论文用这个指标评估建图结果；另外一些论文认为 建图效果正比于定位效果，所以可以用定位效果来评估。有些算法会对地图进行细化，所以也需要单独评估地图效果的指标。

###
在ubuntu下可以用什么打开pcap数据呀？Wireshark

###
想在lego-loam里面增加激光强度信息，处理激光退化问题，请问有这方面的开源代码或者文献之类的可以参考吗？想利用强度信息进行配准或回环检测、《robust place recoginition》《visual place recoginition using LiDAR intensity information》《intensity image-based lidar fiducial marker system》

###
88微lX4N2sQCAc4， https://m.tb.cn/h.fyWxb3J  CZ3457 CH104M CH110 ROS陀螺仪模块加速度计9轴姿态传感器 倾角模块惯导.视觉里的imu都是很廉价的消费级，不知道激光对imu的要求如何？超核电子的CH110，用过还行

###
有关于三维雷达重定位研究的论文嘛？前面有个bow3d，scancontext也还行

###
Sophus::SE3 怎么转成Eigen::Matrix4d或者四元数呢？成员函数rotiationMatrix；转出来自己拼接一下

###
请问各位大佬可以二次开发的小型无人机整机有推荐的品牌吗？预算1-2万。搜 阿木实验室，我用过他们的

###
有没有搞过nerf相关的，这个方向怎么样？感觉可以voxelnet/pointpillar + netvlad， 做数据闭环的3d真值。

###
将点云投影到航拍地图是怎么操作的呢？是通过将点云图导出为KML或KMZ格式再导入到谷歌地图？图新地球，localspaceviewer，也试试这个，originlab

###
![pic](./img/李代数旋转向量的表示方法.png）
红色部分表示旋转矩阵,李代数旋转向量的表示方法
![pic](./img/李代数旋转向量的表示方法2.png）

###
USVinland数据集,做无人船. 用的是毫米波雷达,我试过水面用VINS，效果不大好.
ORB精度还是比vins高，毕竟水面上的干扰点没那么容易识别到.
相机放船上，离岸近一些有纹理就可以跑,湖面要是波光粼粼,得水岸分割一下去除水面点

###
大佬们有没有用自己的单目+imu跑过orbslam3啊，我的一直在不断的初始化。你是不是相机和imu外参没弄对？应该不会，vins可以跑，不知道跟相机imt时间同步有很大关系。Vins对外参会在线标定 Orb3没有，另外你可以把初始化的一些check放宽。

###
![pic](./img/尺度.jpg）
B，把尺度消掉，变成齐次方程，然后最小奇异值对应的右奇异向量即为所求

###
单目相机 不同场景对同一物体成像比例不同，会影响重建结果的尺度，要靠不同远近的成像大小来算出 深度距离的吧。.双目相机要有焦距f和基线b  才能三角化出深度

###
大佬们，有人用NDT做过激光里程计吗？hdl_graph_slam 了解一下。

我这边用的是固态雷达，不知道怎么提取特征点，不做回环，我们现在的方案是将关键帧保存下来，做成local_map，然后将每帧激光数据与local_map做NDT的匹配。但是最初始时，local_map里的点云数据太少了，导致定位精度不够。所以想通过对点云数据提取特征点来提升定位精度。

个人感觉绕远路了。NDT本身就是通过不提取特征和大尺度体素化进行加速的，你想要用NDT，就不该从特征点角度考虑，想要特征点，就不该用NDT。

融合二者提高鲁棒性的论文，优化框架融合提高鲁棒性，指把NDT的优化目标函数 和 LOAM的优化目标函数都放在一个表达式里进行非线性优化。最后反正总时间小于100ms，结论是可以实时运行

NDT的运行速度是多少？我把lio-sam的前端抽出来以后，差不多40ms。用NDT_OMP会快一点。喜欢NDT可以看下LiTAMIN2，论文速度500Hz

###
用lio-sam跑自己的数据集，一旋转后，图就飞了。是什么原因呢？已标定外参，有可能是IMU的频率不是100HZ，试着慢点转

###
imu加速度计标定出来零偏是0，应该怎么加到lio-sam中，一加就会报错。弄一个e-10试试

###
imu坐标系最好和雷达的一致，主要是xy轴；试下浙大的标定工具lidar-imu-calib；把imu贴在lidar正上方也是可以用的

###
我添加了GPS数据，刚跑了5m左右的时候地图突然整个旋转了一个方向，然后就正常了，但是会出现这个视频中一闪一闪的情况，是不是TF做的不对，可不可以解答一下。

发布了两个同名的tf会这样。

应该是你想使用gps的坐标系作为你slam的坐标系，原本slam坐标系是初始桢位姿，所以这里的跳变应该就是将slam系切到gps坐标系下，liosam的github上有一个例程视频就有一个跳变，使用gps那个。

如果让GPS和激光雷达的x,y坐标和yaw角几乎重合的话，是不是就不会有这个跳变了？你得有高精度gps测量器材，还是有点难吧

###
![pic](./img/gtsam.jpg）
![pic](./img/gtsam2.jpg）
是不是gtsam的因子的协方差给0了？可以给个极小值。

我换了个新的imu，imu频率高就报这个错，频率低于100就没问题。跑之前的数据集也没问题，

有人说要改成QR分解改了还是有，可以改改liosam里面imu dt那个位置不知道有没有用。liosam里如果预积分dt小于0，它会置为1/500，改成自己的频率。

###
就只有initial guess和去畸变的作用。我实测发现，把imuPreintegration去掉，跑自己的数据更稳定，基本都能建图成功，但使用的话失败概率挺高的。initial guess应该也不太用得着搞预积分这一套了，直接使用imu位姿递推就够了。那说明你imu的外参可能不太准，基本情况下 用会比不用好一些

###
外参是CAD给的，也标定过，大差不差，但没做时间同步。我看issue里有不少相关的问题，跑自己的数据就是不理想。这里加imu预积分能好多少，这块大家有做过量化比较吗？查看一下imu的频率，频率特别低的话反不如LOAM

###
请问一下大佬 lvi-sam里不是关联相机和雷达点的深度，关联成功的才用于位姿计算吗？那这样不会可用的点很少嘛 还是赋权重，关联成功的高一点呢？

可用的点是不多，不过这个应该是锦上添花作用，毕竟即使没有深度，依赖ba也可以恢复3d深度

请问一下大佬 lvi-sam这种多传感器融合的，视觉可用的点是不是和激光雷达可用的点云比起来少很多？视觉特征点是稀疏的，激光点云是稠密的，所以这么说也没啥问题

###
 现在的vslam算法中单目，双目还是RGB-D用的更多一点啊 ？看你应用场景吧，比如室内就可以用rgbd，室外就不用。比较小的室内，RGBD基本够用吧。室外应该必须带lidar吧？不然单目没法应用吧

###
Nerf可以落地吗，看论文效果是真的不错。落地应该需要时间吧，但目前学术圈比较认可。秦通博士在知乎写了一个综述，大家可以关注一下。Nerf有没有推荐的论文呢，有开源代码，比较经典的那种？你看秦通这个综述就行，上面有很多参考文献

###
请问大家有推荐多传感器融合定位方法相关的综述论文吗？
ekf eskf之类的都可以，对应名义量或者误差量；

激光雷达和imu在LIO-SAM里面是用因子图对吧。在fast-LIO是用IEKF， fast lio就没有后端图优化，就相当于是个LIO里程计

###
lvisam中特征关联的时候，为什么用到了一个单位球呢？而不是用一个归一化平面那样？为了找最近的激光点

###
大家跑liosam的时候，有出现过imu数据丢包导致建图飘逸的情况吗，有没有什么好的解决方案呢？标定好外参，做好时间同步，如果是imu噪声大，可以适当上调imu的内参，降低对其置信度。

比如100hz的imu，在1.2秒到1.3秒丢了10个数据，也就是说1.2秒到1.3秒时是没有数据的，但是这10个数据会在1.3秒时被补上，也就是说1.3秒到1.4秒有20个数据，前10个数据是本该属于1.2-1.3s时的数据，但是却被打上了1.3s的时间戳…imu噪声还好，就是数据丢包之后卡在一堆了。那就是有很严重的通讯缓冲延迟，最好imu信息里自带时间戳，这样估计还好。现在很多imu节点的时间戳都是在收到数据，在数据打包好的时候再打戳的，这中间可能因为一些通讯延时，程序处理时延，导致跟真实采集时刻相去甚远了

###
大多数的多传感器融合方案都会做时间戳同步吧？最好外部硬件硬触发，组合导航可以给传感器授时。有个GPRS信号线来着，不过需要你的传感器支持外部硬触发。激光雷达应该都支持

###
请教一下，无法识别可见光的相机怎么去标定？可以用特殊反光材料制作标定板，加个辅助光源，增强相机相机能接受波段的光，这样相机采集到的图像除了反光区域外，其余地方基本上都是黑的

###
![pic](./img/deep.png）
深度z=fb/d=900*0.5/20=22.5m

###
![pic](./img/orb2.png）
orb-slam2,如果在这个地方加一个掩膜,就可以在掩膜区域不提取特征点。

###
![pic](./img/deep2.png）
z=fb/d,d是左右图横坐标之差，相似三角形的应用.

###
我们群里有没有做点云配准的呀，之前老师让我学的slam现在让我做点云配准，不是很了解。
配准有很多变种的算法，去研究下就行，我觉得能利用新的配准方法和技巧解决某个领域关键问题就行，slam和三维重建，那是两个包含前后端的大问题。配准挺多的，变成二位，用强度特征也可以做配准。NDT一堆变种，瞎搞搞弄个中文简单。

place recognition很多用dl描述子。

直接在三维点云上做的place recognition可以看pointnetVLAD，LSD-Net等等

可以把当前点云降为成2d，2d可以有多维度特征，当作输入到网络。多个角度不同位置做训练。

做全局描述子的可以看m2dp，lidar iris等等。

多传感器融合挺多成熟的方案，工程化的东西比较多。3个传感器，对算力精度研究很高。而且优化难度很大，引入一种不同数据类型的传感器，必须会带来问题，置信度。

多传感器融合做检测的前融合挑战很大，效果会变得更差，硬件时间同步太难了，都是死在标定和同步上

###
![pic](./img/单应.png）
b,当场景中特征点都落在同一平面上，如墙，地面等，可用单应矩阵估计运动；就是一个平面一个单应变换。因为纯旋转的时候位移为零，对积几何无论取何值均满足。ORB_SLAM中的单目初始化就用到了单应矩阵解决纯旋转的情况

###
![pic](./img/初始化.png）

###
本质矩阵是怎么分解成RT的啊，公式没看懂怎么来的？SVD分解，VD和这些公式之间省略了推导
![pic](./img/rt.png）

###
![pic](./img/投影.png）
B，图优化里顶点是待优化的变量，边是约束条件关系；题目中相机位姿完全可靠不用优化，所以只用优化三维点坐标

###
![pic](./img/index.jpg）

###
![pic](./img/cv.jpg）
opencv版本是3.2.0，安ros自带的那个。这是类型萃取机，版本太低了没法编译通过，至少要3.3以上，opencv3.2标准应该不支持这特性。

###
有没有推荐的室内双目数据集？https://t.zsxq.com/02ujAmQnA 看看星球里这个回答，很全的slam数据集。

###
groundtruth.txt文件中真实的位姿个数都大于图像rgb的图像数量，两个文件的时间戳也不一样，请问一下，怎么样评价自己的算法精度，理论上来说，每一张图片对应着一个位姿，但是真实的位姿对不上。

需要时间戳对齐的。把rgb图像的时间戳和groundtruth的对齐，evo可以操作

###
PTAM和orbslam为稀疏地图，LSD-SLAM为半稠密地图。KinectFusion为稠密地图。

###
固态雷达的视场角较小，基于特征的话，场景特征不多会影响较大。可借鉴Fast-lio2，提供了livox40和horizon的接口。

###
多传感器融合课程的标定工具。速腾16先和1000元左右的9轴IMU。效果可以。

###
有没有朋友知道orbslam自带的评估，最后这个数据是什么含义，为啥真值还会有绝对轨迹误差?https://github.com/weichnn/Evaluation_Tools
可以看看你这工具的官网介绍

###
我把 Eigen::Matrix4d 的变量转换成 Sophus::SE3d，为什么有的可以，有的就报错?se3左上角是so3,需要满足正交性质，你随便找个矩阵，不一定能通过的.sophus 好像是e-6还是-9就不过检查了，非常郁闷，而且做normalize丢进去都没用.你尝试转四元数归一化，然后再把rtmtx丢进去试试。sophus支持传四元数。转四元数，再丢进sophus就没问题了

构造 SO3 的矩阵必须正交，还要正定。


###
![pic](./img/ch8.jpg）
在十四讲ch8直接法中使用g2o优化出现的.看起来是顶点定义的时候那个索引 id 有问题，越界了，查一查这块顶点或边的 id


###
有没有小伙伴标定过zed2，想请教一下，370帧图像只有6帧提取了角点，另外一个数据集压根没有提取角点。我在zed2的common.yaml文件里设置分辨率为HD720，按理应该是1280 720，但是最后给的标定文件里是640 360，而且我使用 rostopic echo打印相机参数也是640 480。我选任何一个分辨率，最后输出的结果都是我选的一半。

介绍现状：公司发展，产品简介 -->
提问 -->
了解对方业务(需提前背调了解)进展 -->
询问业务量以上下游备货生产、确定价格