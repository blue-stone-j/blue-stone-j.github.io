---
layout: post
title:  "cuda"
date:   2022-11-20 08:30:00 +0800
categories: [Lan]
excerpt: cuda
tags:
  -  
  - cuda
  - 
---

# 关于库的`compile with cuda`



### flann
1. 



1. 由于硬件设计，调度会以一整个线程束为单位进行。因此应该充分利用线程束中的所有线程进行计算，以尽可能少的线程束的数量，同时提高硬件的使用率。
2. 在CPU编程中，我们学过空间局部性与时间局部性。对全局内存的访问要尽量进行合并访问与存储，让对内存的访问更加集中(访问的内存地址尽量连续)，这样才能达到最大的带宽。
3. 那些可以到达__syncthreads()的线程需要其他可以到达该点的线程，而不是等待块内所有其他线程。

NVVP是可视化的非常实用的工具。


解决虚假依赖的最好办法就是多个工作队列，这样就从根本上解决了虚假依赖关系，Hyper-Q就是这种技术，32个硬件工作队列同时执行多个流，这就可以实现所有流的并发，最小化虚假依赖。

# 内存模型

时间局部性，就是一个内存位置的数据某时刻被引用，那么在此时刻附近也很有可能被引用，随时间流逝，该数据被引用的可能性逐渐降低。
空间局部性，如果某一内存位置的数据被使用，那么附近的数据也有可能被使用。

### 寄存器
在核函数中定义的有常数长度的数组也是在寄存器中分配地址的。寄存器对于每个线程是私有的，寄存器变量的声明周期和核函数一致。寄存器是SM中的稀缺资源，一个线程如果使用更少的寄存器，那么就会有更多的常驻线程块，SM上并发的线程块越多。、

关键字 `__lauch_bounds__(maxThreadaPerBlock,minBlocksPerMultiprocessor)` `__share__`

编译选项中加入 `-maxrregcount=32`

核函数中符合存储在寄存器中但不能进入被核函数分配的寄存器空间中的变量将存储在本地内存中。本地内存实质上是和全局内存一样在同一块存储区域当中的，其访问特点——高延迟，低带宽。对于2.0以上的设备，本地内存存储在每个SM的一级缓存，或者设备的二级缓存上。

### 共享内存
一个线程块使用的共享内存过多，导致更过的线程块没办法被SM启动，这样影响活跃的线程束数量。共享内存在核函数内声明，生命周期和线程块一致。使用`__syncthreads`可缓解竞争。

声明一个二维浮点数共享内存数组的方法是: `__shared__ float a[size_x][size_y];`

填充静态声明的共享内存: `_shared__ int tile[BDIMY][BDIMX+IPAD];`。唯一要注意的就是索引，当填充了以后我们的行不变，列加宽了，所以索引的时候要相应改变。


动态声明一个共享内存数组，可以使用extern关键字，并在核函数启动时添加第三个参数。注意，动态声明只支持一维数组。
```C++
extern __shared__ int tile[];
kernel<<<grid,block,isize*sizeof(int)>>>(...); // isize就是共享内存要存储的数组的大小。比如一个十个元素的int数组，isize就是10.
```

当多个线程要访问一个存储体的时候，冲突就发生了，注意这里是说访问同一个存储体，而不是同一个地址.???

访问模式查询: 可以通过以下语句，查询是4字节还是8字节：
```C++
cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig * pConfig);
```
在可以配置的设备上，可以用下面函数来配置新的存储体大小：
```C++
cudaError_t cudaDeviceSetShareMemConfig(cudaSharedMemConfig config);
```

每个SM上有64KB的片上内存，共享内存和L1共享这64KB，并且可以配置。CUDA为配置一级缓存和共享内存提供以下两种方法：
按设备进行配置(`cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig);`)
按核函数进行配置(`cudaError_t cudaFuncSetCacheConfig(const void* func,enum cudaFuncCacheca cheConfig);`)

一级缓存和共享内存都在同一个片上，但是行为大不相同，共享内存靠的的是存储体来管理数据，而L1则是通过缓存行进行访问。我们对共享内存有绝对的控制权，但是L1的删除工作是硬件完成的。

同步基本方法：障碍、内存栅栏。障碍是所有调用线程等待其余调用线程达到障碍点。内存栅栏，所有调用线程必须等到全部内存修改对其余线程可见时才继续进行。

弱排序内存模型: 核函数内连续两个内存访问指令，如果独立，其不一定哪个先被执行。Volatile修饰符声明一个变量，则对该变量同一时间只能执行一个指令。

##### 内存栅栏???
1. 线程块内
`void __threadfence_block();`
保证同一块中的其他线程对于栅栏前的内存写操作可见

2. 网格级内存栅栏
`void __threadfence();`
挂起调用线程，直到全局内存中所有写操作对相同的网格内的所有线程可见

3. 系统级栅栏，夸系统，包括主机和设备，
`void __threadfence_system();`
挂起调用线程，以保证该线程对全局内存，锁页主机内存和其他设备内存中的所有写操作对全部设备中的线程和主机线程可见。

volatile修饰符声明一个变量，防止编译器优化，volatile声明的变量始终在全局内存中。

##### 共享内存的数据布局
在CPU中，如果用循环遍历二维数组，尤其是双层循环的方式，我们倾向于内层循环对应x，因为这样的访问方式在内存中是连续的。但是GPU的共享内存并不是线性的，而是二维的，分成不同存储体的，并且，并行也不是循环。

我们的数据是按照行放进存储体中的。所以`x[threadIdx.y][threadIdx.x]`这种访问方式是最优的，threadIdx.x在线程束中体现为连续变化的，而对应到共享内存中也是遍历共享内存的同一行的不同列。



### 常量内存
关键字 `__constant__`

常量内存在核函数外，全局范围内声明，并对同一编译单元中的所有核函数可见。主机端代码可以初始化常量内存的,不能被核函数修改。初始化函数如下：
```C++
cudaError_t cudaMemcpyToSymbol(const void* symbol,const void *src,size_t count);
```
当线程束中所有线程都从相同的地址取数据时，常量内存表现较好，但是如果不同的线程取不同地址的数据，常量内存就不那么好了，因为常量内存的读取机制是：
一次读取会广播给所有线程束内的线程。

，只读缓存对于分散访问的更好。当所有线程读取同一地址的时候常量缓存最好，只读缓存这时候效果并不好，只读换粗粒度为32.

### 纹理内存
???
### 全局内存
一般在主机端代码里定义，也可以在设备端定义，不过需要加修饰符，只要不销毁，是和应用程序同生命周期的。全局内存访问是对齐。

### GPU缓存
GPU缓存不可编程，其行为出厂是时已经设定好了。GPU上有4种缓存：一级缓存、二级缓存、只读常量缓存、只读纹理缓存。每个SM都有一个一级缓存，所有SM公用一个二级缓存。每个SM有一个只读常量缓存，只读纹理缓存，它们用于设备内存中提高来自于各自内存空间内的读取性能。

### 静态全局内存

### 内存管理

在数据传输之前，CUDA驱动会锁定页面，或者直接分配固定的主机内存，将主机源数据复制到固定内存上，然后从固定内存传输数据到设备上。下面函数用来分配固定内存：
```C++
cudaError_t cudaMallocHost(void ** devPtr,size_t count);
```
分配count字节的固定内存，这些内存是页面锁定的，可以直接传输到设备的. 固定的主机内存释放使用：
```C++
cudaError_t cudaFreeHost(void *ptr);
```
固定内存的释放和分配成本比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。

### 零拷贝内存

通过以下函数创建零拷贝内存：
```C++
cudaError_t cudaHostAlloc(void ** pHost,size_t count,unsigned int flags);
```
cudaHostAllocDefalt、cudaHostAllocPortable、cudaHostAllocWriteCombined、cudaHostAllocMapped
cudaHostAllocDefalt和cudaMallocHost函数一致，cudaHostAllocPortable函数返回能被所有CUDA上下文使用的固定内存，cudaHostAllocWriteCombined返回写结合内存，在某些设备上这种内存传输效率更高。cudaHostAllocMapped产生零拷贝内存。???

零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过pHost直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存，方法是：
```C++
cudaError_t cudaHostGetDevicePointer(void ** pDevice,void * pHost,unsigned flags);
```
pDevice就是设备上访问主机零拷贝内存的指针了。此处flag必须设置为0，具体内容后面有介绍。


### 统一虚拟寻址

设备架构2.0以后，`cudaHostGetDevicePointer`基本上可以不再使用

### 统一内存寻址
CUDA6.0出现统一内存寻址。
```C++
cudaError_t cudaMallocManaged(void ** devPtr,size_t size,unsigned int flags=0);
```
CUDA6.0中设备代码不能调用cudaMallocManaged，只能主机调用，所有托管内存必须在主机代码上动态声明，或者全局静态声明.
页面故障???

### 线程束洗牌指令




# 流和事件


事件通过下面指令添加到CUDA流：
```C++
cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0);
```

CDUA提供了一种控制事件行为和性能的函数：
```C++
cudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags);
```

空流不需要显式声明，而是隐式的，他是阻塞的，跟所有阻塞流同步。默认创建的非空流是阻塞版本。

### 隐式同步
忽略隐式同步会造成性能下降。隐式同步常出现在内存操作上，比如锁页主机内存分布、设备内存分配、设备内存初始化、同一设备两地址之间的内存复制、一级缓存和共享内存配置修改。这些操作都要时刻小心，因为他们带来的阻塞非常不容易察觉。

### 显式同步
常见的显式同步有同步设备、同步流、同步流中的事件、使用事件跨流同步。
1. 阻塞主机线程，直到设备完成所有操作：
```C++
cudaError_t cudaDeviceSynchronize(void);
```
这个函数我们前面常用，但是尽量少用，这个会拖慢效率。
2. 流版本的，我们可以同步流，使用下面两个函数：
```C++
cudaError_t cudaStreamSynchronize(cudaStream_t stream);
cudaError_t cudaStreamQuery(cudaStream_t stream);
```
cudaStreamSynchronize会阻塞主机，直到流完成。cudaStreamQuery则是立即返回，如果查询的流执行完了，那么返回cudaSuccess否则返回cudaErrorNotReady。

事件，事件的作用就是在流中设定一些标记用来同步，和检查是否执行到关键点位（事件位置），也是用类似的函数
```C++
cudaError_t cudaEventQuery(cudaEvent_t event); // 异步
cudaError_t cudaEventSynchronize(cudaEvent_t event); // 同步
```
这两个函数的性质和上面的非常类似。
事件提供了一个流之间同步的方法：
```C++
cudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event);
```
这条命令的含义是，指定的流要等待指定的事件，事件完成后流才能继续，这个事件可以在这个流中，也可以不在，当在不同的流的时候，这个就是实现了跨流同步。

### 用环境变量调整流行为
对于Linux系统中，修改方式如下：
```Bash
#For Bash or Bourne Shell:
export CUDA_DEVICE_MAX_CONNECTIONS=32
#For C-Shell:
setenv CUDA_DEVICE_MAX_CONNECTIONS 32
```
另一种修改方法是直接在程序里写，这种方法更好用通过底层驱动修改硬件配置：
```Bash
setenv("CUDA_DEVICE_MAX_CONNECTIONS", "32", 1);
```

### 创建流间依赖关系
声明成 cudaEventDisableTiming 的同步事件：
```C++
cudaEvent_t * event=(cudaEvent_t *)malloc(n_stream*sizeof(cudaEvent_t));
for(int i=0;i<n_stream;i++)
{
    cudaEventCreateWithFlag(&event[i],cudaEventDisableTiming);
}
```
在流中加入指令:
```C++
for(int i=0;i<n_stream;i++)
{
    kernel_1<<<grid,block,0,stream[i]>>>();
    kernel_2<<<grid,block,0,stream[i]>>>();
    kernel_3<<<grid,block,0,stream[i]>>>();
    kernel_4<<<grid,block,0,stream[i]>>>();
    cudaEventRecord(event[i],stream[i]);
    cudaStreamWaitEvent(stream[n_stream-1],event[i],0);
}
```
这时候，最后一个流（第5个流）都会等到前面所有流中的事件完成，自己才会完成.


### 
Fermi架构和Kepler架构下有两个复制引擎队列，也就是数据传输队列，一个从设备到主机，一个从主机到设备。所以读取和写入是不经过同一条队列的，这样的好处就是这两个操作可以重叠完成了，

数据传输使用异步方式，注意异步处理的数据要声明称为固定内存，不能是分页的，如果是分页的可能会出现未知错误。